{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to implement handoffs between agents\n",
    "- https://langchain-ai.github.io/langgraph/how-tos/agent-handoffs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement handoffs using `Command`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import MessagesState, StateGraph, START\n",
    "from langgraph.types import Command\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def transfer_to_multiplication_expert():\n",
    "    \"\"\"Ask multiplication agent for help.\"\"\"\n",
    "    # This tool is not returning anything: we're just using it\n",
    "    # as a way for LLM to signal that it needs to hand off to another agent\n",
    "    # (See the paragraph above)\n",
    "    return\n",
    "\n",
    "\n",
    "@tool\n",
    "def transfer_to_addition_expert():\n",
    "    \"\"\"Ask addition agent for help.\"\"\"\n",
    "    return\n",
    "\n",
    "\n",
    "def addition_expert(\n",
    "    state: MessagesState,\n",
    ") -> Command[Literal[\"multiplication_expert\", \"__end__\"]]:\n",
    "    system_prompt = (\n",
    "        \"You are an addition expert, you can ask the multiplication expert for help with multiplication. \"\n",
    "        \"Always do your portion of calculation before the handoff.\"\n",
    "    )\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    ai_msg = model.bind_tools([transfer_to_multiplication_expert], parallel_tool_calls=False).invoke(messages)\n",
    "    # If there are tool calls, the LLM needs to hand off to another agent\n",
    "    if len(ai_msg.tool_calls) > 0:\n",
    "        tool_call_id = ai_msg.tool_calls[-1][\"id\"]\n",
    "        # NOTE: it's important to insert a tool message here because LLM providers are expecting\n",
    "        # all AI messages to be followed by a corresponding tool result message\n",
    "        tool_msg = {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": \"Successfully transferred\",\n",
    "            \"tool_call_id\": tool_call_id,\n",
    "        }\n",
    "        return Command(\n",
    "            goto=\"multiplication_expert\", update={\"messages\": [ai_msg, tool_msg]}\n",
    "        )\n",
    "\n",
    "    # If the expert has an answer, return it directly to the user\n",
    "    return {\"messages\": [ai_msg]}\n",
    "\n",
    "\n",
    "def multiplication_expert(\n",
    "    state: MessagesState,\n",
    ") -> Command[Literal[\"addition_expert\", \"__end__\"]]:\n",
    "    system_prompt = (\n",
    "        \"You are a multiplication expert, you can ask an addition expert for help with addition. \"\n",
    "        \"Always do your portion of calculation before the handoff.\"\n",
    "    )\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    ai_msg = model.bind_tools([transfer_to_addition_expert]).invoke(messages)\n",
    "    print(\"ai_msg:::\", ai_msg)\n",
    "    print(\"ai_msg.content:::\", ai_msg.content)\n",
    "    if len(ai_msg.tool_calls) > 0:\n",
    "        tool_call_id = ai_msg.tool_calls[-1][\"id\"]\n",
    "        tool_msg = {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": \"Successfully transferred\",\n",
    "            \"tool_call_id\": tool_call_id,\n",
    "        }\n",
    "        return Command(goto=\"addition_expert\", update={\"messages\": [ai_msg, tool_msg]})\n",
    "\n",
    "    return {\"messages\": [ai_msg]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"addition_expert\", addition_expert)\n",
    "builder.add_node(\"multiplication_expert\", multiplication_expert)\n",
    "# we'll always start with the addition expert\n",
    "builder.add_edge(START, \"addition_expert\")\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "\n",
    "def pretty_print_messages(update):\n",
    "    if isinstance(update, tuple):\n",
    "        ns, update = update\n",
    "        # skip parent graph updates in the printouts\n",
    "        if len(ns) == 0:\n",
    "            return\n",
    "\n",
    "        graph_id = ns[-1].split(\":\")[0]\n",
    "        print(f\"Update from subgraph {graph_id}:\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    for node_name, node_update in update.items():\n",
    "        print(f\"Update from node {node_name}:\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        for m in convert_to_messages(node_update[\"messages\"]):\n",
    "            m.pretty_print()\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node addition_expert:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Let's calculate it step by step:\n",
      "\n",
      "1. First, add \\(3 + 5\\):\n",
      "   \\[\n",
      "   3 + 5 = 8\n",
      "   \\]\n",
      "\n",
      "2. Now, multiply that result by 12:\n",
      "   \\[\n",
      "   8 * 12\n",
      "   \\]\n",
      "\n",
      "I'll reach out to the multiplication expert for assistance with the multiplication.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"6\"}}\n",
    "for chunk in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"what's (3 + 5) * 12\")]}, config=config\n",
    "):\n",
    "\n",
    "    pretty_print_messages(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement handoffs using tools\n",
    "Explicitly defined custom handoffs in each of the agent nodes.\n",
    "\n",
    "Few important considerations when implementing handoff tools:\n",
    "1. Single each agent is a subgraph node in another graph, and tools will be called in one of the agent subgraph nodes, we need to specify `graph=Command.PARENT` in the `Command` to navigate outside of the subgraph.\n",
    "2. We can optionally specify a state update that will be applied to the parent graph before the next agent is called\n",
    "3. We can optionally provide the following to the tool\n",
    "- graph state (`InjectedState`)\n",
    "- graph long-term memory (`InjectedMemory`)\n",
    "- current tool call ID (`InjectedToolCallId`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a handoff tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.tools.base import InjectedToolCallId\n",
    "from langgraph.prebuilt import InjectedState\n",
    "\n",
    "\n",
    "def make_handoff_tool(*, agent_name: str):\n",
    "    \"\"\"Create a tool that can return handoff via a Command\"\"\"\n",
    "    tool_name = f\"transfer_to_{agent_name}\"\n",
    "\n",
    "    @tool(tool_name)\n",
    "    def handoff_to_agent(\n",
    "        # # optionally pass current graph state to the tool (will be ignored by the LLM)\n",
    "        state: Annotated[dict, InjectedState],\n",
    "        # optionally pass the current tool call ID (will be ignored by the LLM)\n",
    "        tool_call_id: Annotated[str, InjectedToolCallId],\n",
    "    ):\n",
    "        \"\"\"Ask another agent for help.\"\"\"\n",
    "        tool_message = {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": f\"Successfully transferred to {agent_name}\",\n",
    "            \"name\": tool_name,\n",
    "            \"tool_call_id\": tool_call_id,\n",
    "        }\n",
    "        return Command(\n",
    "            # navigate to another agent node in the PARENT graph\n",
    "            goto=agent_name,\n",
    "            graph=Command.PARENT,\n",
    "            # This is the state update that the agent `agent_name` will see when it is invoked.\n",
    "            # We're passing agent's FULL internal message history AND adding a tool message to make sure\n",
    "            # the resulting chat history is valid. See the paragraph above for more information.\n",
    "            update={\"messages\": state[\"messages\"] + [tool_message]},\n",
    "        )\n",
    "\n",
    "    return handoff_to_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using with a custom agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import MessagesState, StateGraph, START\n",
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "def make_agent(model, tools, system_prompt=None):\n",
    "    model_with_tools = model.bind_tools(tools, parallel_tool_calls=False)\n",
    "    tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "    def call_model(state: MessagesState) -> Command[Literal[\"call_tools\", \"__end__\"]]:\n",
    "        messages = state[\"messages\"]\n",
    "        if system_prompt:\n",
    "            messages = [{\"role\": \"system\", \"content\": system_prompt}] + messages\n",
    "\n",
    "        response = model_with_tools.invoke(messages)\n",
    "        if len(response.tool_calls) > 0:\n",
    "            return Command(goto=\"call_tools\", update={\"messages\": [response]})\n",
    "\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    # NOTE: this is a simplified version of the prebuilt ToolNode\n",
    "    # If you want to have a tool node that has full feature parity, please refer to the source code\n",
    "    def call_tools(state: MessagesState) -> Command[Literal[\"call_model\"]]:\n",
    "        tool_calls = state[\"messages\"][-1].tool_calls\n",
    "        results = []\n",
    "        for tool_call in tool_calls:\n",
    "            tool_ = tools_by_name[tool_call[\"name\"]]\n",
    "            tool_input_fields = tool_.get_input_schema().model_json_schema()[\n",
    "                \"properties\"\n",
    "            ]\n",
    "\n",
    "            # this is simplified for demonstration purposes and\n",
    "            # is different from the ToolNode implementation\n",
    "            if \"state\" in tool_input_fields:\n",
    "                # inject state\n",
    "                tool_call = {**tool_call, \"args\": {**tool_call[\"args\"], \"state\": state}}\n",
    "\n",
    "            tool_response = tool_.invoke(tool_call)\n",
    "            if isinstance(tool_response, ToolMessage):\n",
    "                results.append(Command(update={\"messages\": [tool_response]}))\n",
    "\n",
    "            # handle tools that return Command directly\n",
    "            elif isinstance(tool_response, Command):\n",
    "                results.append(tool_response)\n",
    "\n",
    "        # NOTE: nodes in LangGraph allow you to return list of updates, including Command objects\n",
    "        return results\n",
    "\n",
    "    graph = StateGraph(MessagesState)\n",
    "    graph.add_node(call_model)\n",
    "    graph.add_node(call_tools)\n",
    "    graph.add_edge(START, \"call_model\")\n",
    "    graph.add_edge(\"call_tools\", \"call_model\")\n",
    "\n",
    "    return graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies two numbers.\"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node call_model:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  add (call_9c4j6l4gSZxYfCMZeY5GnJL4)\n",
      " Call ID: call_9c4j6l4gSZxYfCMZeY5GnJL4\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 5\n",
      "\n",
      "\n",
      "Update from node call_tools:\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: add\n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "Update from node call_model:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_n9zt1sUOgBB6RmrtFybikTY0)\n",
      " Call ID: call_n9zt1sUOgBB6RmrtFybikTY0\n",
      "  Args:\n",
      "    a: 8\n",
      "    b: 12\n",
      "\n",
      "\n",
      "Update from node call_tools:\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "96\n",
      "\n",
      "\n",
      "Update from node call_model:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The result of \\((3 + 5) * 12\\) is \\(96\\).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = make_agent(model, [add, multiply])\n",
    "\n",
    "for chunk in agent.stream({\"messages\": [(\"user\", \"what's (3 + 5) * 12\")]}):\n",
    "    pretty_print_messages(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "addition_expert = make_agent(\n",
    "    model,\n",
    "    [add, make_handoff_tool(agent_name=\"multiplication_expert\")],\n",
    "    system_prompt=\"You are an addition expert, you can ask the multiplication expert for help with multiplication.\",\n",
    ")\n",
    "multiplication_expert = make_agent(\n",
    "    model,\n",
    "    [multiply, make_handoff_tool(agent_name=\"addition_expert\")],\n",
    "    system_prompt=\"You are a multiplication expert, you can ask an addition expert for help with addition.\",\n",
    ")\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"addition_expert\", addition_expert)\n",
    "builder.add_node(\"multiplication_expert\", multiplication_expert)\n",
    "builder.add_edge(START, \"addition_expert\")\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from subgraph addition_expert:\n",
      "\n",
      "\n",
      "Update from node call_model:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  transfer_to_multiplication_expert (call_FcFCFnCWQGjhEQFvTAGDDL4d)\n",
      " Call ID: call_FcFCFnCWQGjhEQFvTAGDDL4d\n",
      "  Args:\n",
      "\n",
      "\n",
      "Update from subgraph multiplication_expert:\n",
      "\n",
      "\n",
      "Update from node call_model:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_SZBUFUhtTNGGJnkBNBxVB4ET)\n",
      " Call ID: call_SZBUFUhtTNGGJnkBNBxVB4ET\n",
      "  Args:\n",
      "    a: 8\n",
      "    b: 12\n",
      "\n",
      "\n",
      "Update from subgraph multiplication_expert:\n",
      "\n",
      "\n",
      "Update from node call_tools:\n",
      "\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "96\n",
      "\n",
      "\n",
      "Update from subgraph multiplication_expert:\n",
      "\n",
      "\n",
      "Update from node call_model:\n",
      "\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The result of \\((3 + 5) * 12\\) is 96.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"what's (3 + 5) * 12\")]}, subgraphs=True\n",
    "):\n",
    "    pretty_print_messages(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using with a pre-built ReAct agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "addition_expert = create_react_agent(\n",
    "    model,\n",
    "    [add, make_handoff_tool(agent_name=\"multiplication_expert\")],\n",
    "    prompt=\"You are an addition expert, you can ask the multiplication expert for help with multiplication.\",\n",
    ")\n",
    "\n",
    "multiplication_expert = create_react_agent(\n",
    "    model,\n",
    "    [multiply, make_handoff_tool(agent_name=\"addition_expert\")],\n",
    "    prompt=\"You are a multiplication expert, you can ask an addition expert for help with addition.\",\n",
    ")\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"addition_expert\", addition_expert)\n",
    "builder.add_node(\"multiplication_expert\", multiplication_expert)\n",
    "builder.add_edge(START, \"addition_expert\")\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"what's (3 + 5) * 12\")]}, subgraphs=True\n",
    "):\n",
    "    pretty_print_messages(chunk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
