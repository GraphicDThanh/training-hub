{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to disable streaming for models that don't support it\n",
    "- https://langchain-ai.github.io/langgraph/how-tos/disable-streaming/.\n",
    "- Some model not support streaming, set `disable_streaming=True` to our model will solve problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "llm = ChatOpenAI(model=\"o1-preview\", temperature=1, disable_streaming=True)\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "\n",
    "def chatbot(state: MessagesState):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "input = {\"messages\": {\"role\": \"user\", \"content\": \"how many r's are in strawberry?\"}}\n",
    "async for event in graph.astream_events(input, version=\"v2\"):\n",
    "    if event[\"event\"] == \"on_chat_model_end\":\n",
    "        print(event[\"data\"][\"output\"].content, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
