{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to stream data from within a tool\n",
    "- https://langchain-ai.github.io/langgraph/how-tos/streaming-events-from-within-tools/\n",
    "- If graph use tools that use LLMs or any other streaming API, you might want to surface partial results. Especially if the tool takes longer time to run.\n",
    "- Technique 1: `stream_mode=\"custom\"` and `get_stream_writer()` method.\n",
    "- Technique 2: `stream_mode=\"messages\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.config import get_stream_writer\n",
    "\n",
    "@tool\n",
    "async def get_items(place: str) -> str:\n",
    "    \"\"\"Use this tool to list items one might find in a place you're asked about.\"\"\"\n",
    "    writer = get_stream_writer()\n",
    "\n",
    "    # this can be replaced with any actual streaming logic that you might have\n",
    "    items = [\"books\", \"penciles\", \"pictures\"]\n",
    "    for chunk in items:\n",
    "        writer({\"custom_tool_data\": chunk})\n",
    "\n",
    "    return \", \".join(items)\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "tools = [get_items]\n",
    "agent = create_react_agent(llm, tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom_tool_data': 'books'}\n",
      "{'custom_tool_data': 'penciles'}\n",
      "{'custom_tool_data': 'pictures'}\n"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What items are in the office?\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "async for chunk in agent.astream(inputs, stream_mode=\"custom\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming LLM tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.messages import AIMessageChunk\n",
    "\n",
    "@tool\n",
    "async def get_items(\n",
    "    place: str,\n",
    "    config: RunnableConfig\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Use this tools to list items one might find in a place you're asked about.\n",
    "    \"\"\"\n",
    "    response = await llm.ainvoke(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Can you tell me what kind of items i might find in the following place: '{place}'. \"\n",
    "                    \"List at least 3 such items separating them by a comma. And include a brief description of each item.\"\n",
    "            }\n",
    "        ],\n",
    "        config\n",
    "    )\n",
    "\n",
    "    return response.content\n",
    "\n",
    "\n",
    "tools = [get_items]\n",
    "agent = create_react_agent(llm, tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure|!| Here| are| three| items| you| might| find| in| a| bedroom|:\n",
      "\n",
      "|1|.| **|Bed|**|:| The| centerpiece| of| a| bedroom|,| typically| consisting| of| a| mattress| and| a| bed| frame|.| It| is| used| for| sleeping|,| resting|,| and| relaxation|.| Beds| come| in| various| sizes|,| such| as| twin|,| full|,| queen|,| and| king|,| and| can| be| adorned| with| pillows| and| bedding| for| comfort|.\n",
      "\n",
      "|2|.| **|D|resser|**|:| A| piece| of| furniture| with| multiple| drawers| used| for| storing| clothing| and| personal| items|.| Dress|ers| often| have| a| flat| surface| on| top| where| items| like| jewelry| boxes|,| photographs|,| or| decorative| items| can| be| placed|.| They| help| keep| the| bedroom| organized| and| tidy|.\n",
      "\n",
      "|3|.| **|Night|stand|**|:| A| small| table| typically| located| beside| the| bed|,| used| to| hold| essentials| like| a| lamp|,| alarm| clock|,| books|,| or| a| glass| of| water|.| Night|stands| provide| convenient| storage| and| surface| space| for| items| you| may| need| within| arm|'s| reach| while| in| bed|.|"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"what items are in the bedroom?\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "async for msg, metadata in agent.astream(\n",
    "    inputs,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if (\n",
    "        isinstance(msg, AIMessageChunk)\n",
    "        and msg.content\n",
    "        # Stream all messages from the tool node\n",
    "        and metadata[\"langgraph_node\"] == \"tools\"\n",
    "    ):\n",
    "        print(msg.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example without LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import json\n",
    "\n",
    "from typing import TypedDict\n",
    "from typing_extensions import Annotated\n",
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "openai_client = AsyncOpenAI()\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "\n",
    "async def stream_tokens(model_name: str, messages: list[dict]):\n",
    "    response = await openai_client.chat.completions.create(\n",
    "        messages=messages, model=model_name, stream=True\n",
    "    )\n",
    "    role = None\n",
    "    async for chunk in response:\n",
    "        delta = chunk.choices[0].delta\n",
    "\n",
    "        if delta.role is not None:\n",
    "            role = delta.role\n",
    "\n",
    "        if delta.content:\n",
    "            yield {\"role\": role, \"content\": delta.content}\n",
    "\n",
    "\n",
    "# this is our tool\n",
    "async def get_items(place: str) -> str:\n",
    "    \"\"\"Use this tool to list items one might find in a place you're asked about.\"\"\"\n",
    "    writer = get_stream_writer()\n",
    "    response = \"\"\n",
    "    async for msg_chunk in stream_tokens(\n",
    "        model_name,\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"Can you tell me what kind of items \"\n",
    "                    f\"i might find in the following place: '{place}'. \"\n",
    "                    \"List at least 3 such items separating them by a comma. \"\n",
    "                    \"And include a brief description of each item.\"\n",
    "                ),\n",
    "            }\n",
    "        ],\n",
    "    ):\n",
    "        response += msg_chunk[\"content\"]\n",
    "        writer(msg_chunk)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[dict], operator.add]\n",
    "\n",
    "\n",
    "# this is the tool-calling graph node\n",
    "async def call_tool(state: State):\n",
    "    ai_message = state[\"messages\"][-1]\n",
    "    tool_call = ai_message[\"tool_calls\"][-1]\n",
    "\n",
    "    function_name = tool_call[\"function\"][\"name\"]\n",
    "    if function_name != \"get_items\":\n",
    "        raise ValueError(f\"Tool {function_name} not supported\")\n",
    "\n",
    "    function_arguments = tool_call[\"function\"][\"arguments\"]\n",
    "    arguments = json.loads(function_arguments)\n",
    "\n",
    "    function_response = await get_items(**arguments)\n",
    "    tool_message = {\n",
    "        \"tool_call_id\": tool_call[\"id\"],\n",
    "        \"role\": \"tool\",\n",
    "        \"name\": function_name,\n",
    "        \"content\": function_response,\n",
    "    }\n",
    "    return {\"messages\": [tool_message]}\n",
    "\n",
    "\n",
    "graph = (\n",
    "    StateGraph(State)\n",
    "    .add_node(call_tool)\n",
    "    .add_edge(START, \"call_tool\")\n",
    "    .compile()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure|!| Here| are| three| items| commonly| found| in| a| bedroom|:\n",
      "\n",
      "|1|.| **|Bed|**|:| A| piece| of| furniture| typically| consisting| of| a| mattress| and| a| frame|,| used| for| sleeping|.| Beds| can| vary| in| size| (|such| as| twin|,| full|,| queen|,| or| king|)| and| can| come| with| various| styles| and| additional| features| like| storage| drawers| or| elevated| frames|.\n",
      "\n",
      "|2|.| **|D|resser|**|:| A| storage| unit| with| multiple| drawers| used| for| organizing| clothing| and| personal| items|.| Dress|ers| often| serve| as| a| surface| for| additional| decorative| items| and| may| also| come| with| a| mirror| attached| or| positioned| above| it|.\n",
      "\n",
      "|3|.| **|Night|stand|**|:| A| small| table| or| cabinet| positioned| beside| the| bed|,| used| to| hold| items| like| lamps|,| alarm| clocks|,| books|,| or| personal| items| for| easy| access| during| the| night|.| Night|stands| can| vary| in| design| and| materials|,| complement|ing| the| bedroom|'s| overall| decor|.|"
     ]
    }
   ],
   "source": [
    "inputs = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"content\": None,\n",
    "            \"role\": \"assistant\",\n",
    "            \"tool_calls\": [\n",
    "                {\n",
    "                    \"id\": \"1\",\n",
    "                    \"function\": {\n",
    "                        \"arguments\": '{\"place\":\"bedroom\"}',\n",
    "                        \"name\": \"get_items\",\n",
    "                    },\n",
    "                    \"type\": \"function\",\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "async for chunk in graph.astream(\n",
    "    inputs,\n",
    "    stream_mode=\"custom\",\n",
    "):\n",
    "    print(chunk[\"content\"], end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
